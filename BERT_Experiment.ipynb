{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import ast\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hc3.csv')\n",
    "human_paragraphs = [''.join(ast.literal_eval(human_paragraph)).replace('\\n', '').split('.') for human_paragraph in list(df['human_answers'])]\n",
    "chatgpt_paragraphs = [''.join(ast.literal_eval(chatgpt_paragraph)).replace('\\n', '').split('.') for chatgpt_paragraph in list(df['chatgpt_answers'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overlapping_sequences(paragraphs, num_sentences):\n",
    "    combined = []\n",
    "    human_combined = []\n",
    "    chatgpt_combined = []\n",
    "    for paragraph in paragraphs:\n",
    "        sentences = paragraph\n",
    "        for i in range(len(sentences) - num_sentences + 1):\n",
    "            combined.append(' '.join(sentences[i:i+num_sentences]).strip())\n",
    "    return combined\n",
    "\n",
    "human_combined = create_overlapping_sequences(human_paragraphs, 3)\n",
    "chatgpt_combined = create_overlapping_sequences(chatgpt_paragraphs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = pd.DataFrame({'text': human_combined})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_df = pd.DataFrame({'text': chatgpt_combined})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = []\n",
    "        for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "            encoded = self.tokenizer.encode_plus(row['text'], add_special_tokens=True, padding='max_length',\n",
    "                                                 truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            self.data.append((encoded['input_ids'], encoded['attention_mask'], row['label']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids, attention_mask, label = self.data[idx]\n",
    "        return {'input_ids': input_ids.squeeze(0), 'attention_mask': attention_mask.squeeze(0), 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation and test sets\n",
    "def train_val_test_split(ai_df, human_df, val_size=0.2, test_size=0.2):\n",
    "    ai_df['label'] = 0\n",
    "    human_df['label'] = 1\n",
    "    data = pd.concat([ai_df, human_df], ignore_index=True, sort=False)\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle the data\n",
    "    split1 = int(len(data) * (1 - (val_size + test_size)))\n",
    "    split2 = int(len(data) * (1 - test_size))\n",
    "    train_data = data[:split1]\n",
    "    val_data = data[split1:split2]\n",
    "    test_data = data[split2:]\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data, val_data, test_data = train_val_test_split(ai_df, human_df, val_size=0.2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay = 0.01)\n",
    "\n",
    "optimizer = Adafactor(model.parameters(), lr=1e-3, relative_step=True)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Create instances of SentimentDataset for train and eval datasets\n",
    "train_dataset = SentimentDataset(train_data, tokenizer, max_length=128)\n",
    "val_dataset = SentimentDataset(val_data, tokenizer, max_length=128)\n",
    "test_dataset = SentimentDataset(test_data, tokenizer, max_length=128)\n",
    "\n",
    "# Save the datasets as serialized objects\n",
    "with open('train_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "with open('val_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(val_dataset, f)\n",
    "\n",
    "with open('test_dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train_dataset.pickle', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "with open('val_dataset.pickle', 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "with open('test_dataset.pickle', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "best_val_accuracy = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "                \n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Add this line to update the learning rate\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        train_preds.extend(predictions.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    train_acc = accuracy_score(train_labels, train_preds)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc*100:.2f}%')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    eval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for batch in eval_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += len(labels)\n",
    "            \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f'Test Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_samples += len(labels)\n",
    "            val_preds.extend(predictions.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    print(f'Val Accuracy: {val_acc*100:.2f}%')\n",
    "    \n",
    "    # Save the best model based on validation accuracy\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "print(f'Best Validation Accuracy: {best_val_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashwathb24\u001b[0m (\u001b[33mnyu-tandon\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'BERT_final.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sweep_config = {\n",
    "    'name': 'bert-sweep',\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        \n",
    "    'name': 'accuracy',\n",
    "    'goal': 'maximize'\n",
    "        \n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'min': 1e-5,\n",
    "            'max': 5e-5\n",
    "    },\n",
    "        'batch_size': {\n",
    "            'values': [16, 32, 64, 128]\n",
    "    },\n",
    "        'num_epochs': {\n",
    "            'values': [3, 5, 10]\n",
    "    }\n",
    "}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='bert-sentiment-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT with wandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gr39xj51\n",
      "Sweep URL: https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y2pxx6ty with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.4297843990743166e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/as16494/HPML_PROJECT/wandb/run-20230514_121448-y2pxx6ty</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/y2pxx6ty' target=\"_blank\">confused-sweep-1</a></strong> to <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/y2pxx6ty' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/y2pxx6ty</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/as16494/.local/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5: 100%|██████████| 3563/3563 [41:29<00:00,  1.43it/s, loss=0.00361] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0036, Train Accuracy: 98.56%\n",
      "Val Accuracy: 99.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 3563/3563 [30:25<00:00,  1.95it/s, loss=0.000656]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0007, Train Accuracy: 99.64%\n",
      "Val Accuracy: 99.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 3563/3563 [27:09<00:00,  2.19it/s, loss=0.000112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.0001, Train Accuracy: 99.75%\n",
      "Val Accuracy: 98.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 3563/3563 [27:10<00:00,  2.18it/s, loss=2.58e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.0000, Train Accuracy: 99.81%\n",
      "Val Accuracy: 98.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 3563/3563 [37:43<00:00,  1.57it/s, loss=0.000372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.0004, Train Accuracy: 99.84%\n",
      "Val Accuracy: 98.78%\n",
      "Best Validation Accuracy: 99.05%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▂▁▁▂</td></tr><tr><td>train_accuracy</td><td>▁▇███</td></tr><tr><td>val_accuracy</td><td>██▁▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.00037</td></tr><tr><td>train_accuracy</td><td>0.99839</td></tr><tr><td>val_accuracy</td><td>0.98781</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-1</strong> at: <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/y2pxx6ty' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/y2pxx6ty</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230514_121448-y2pxx6ty/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ctys20ot with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 4.348854275205252e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/as16494/HPML_PROJECT/wandb/run-20230514_151645-ctys20ot</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/ctys20ot' target=\"_blank\">quiet-sweep-2</a></strong> to <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/sweeps/gr39xj51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/ctys20ot' target=\"_blank\">https://wandb.ai/nyu-tandon/bert-sentiment-classification/runs/ctys20ot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/as16494/.local/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/5: 100%|██████████| 7125/7125 [29:06<00:00,  4.08it/s, loss=0.00535] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0054, Train Accuracy: 98.45%\n",
      "Val Accuracy: 97.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 7125/7125 [29:06<00:00,  4.08it/s, loss=0.0001]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0001, Train Accuracy: 99.49%\n",
      "Val Accuracy: 97.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:   7%|▋         | 480/7125 [01:57<27:06,  4.09it/s, loss=0.000554]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    if config is None:\n",
    "        config = {\n",
    "            'learning_rate': 5e-5,\n",
    "            'num_epochs': 5,\n",
    "            'batch_size': 64\n",
    "        }\n",
    "\n",
    "    # Initialize the tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    # Define the training loop\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = config['num_epochs']\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_preds.extend(predictions.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc*100:.2f}%')\n",
    "\n",
    "        model.eval()\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += len(labels)\n",
    "                val_preds.extend(predictions.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        print(f'Val Accuracy: {val_acc*100:.2f}%')\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        wandb.log({'epoch': epoch, 'loss': loss.item(), 'train_accuracy': train_acc, 'val_accuracy': val_acc})\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_val_accuracy*100:.2f}%')\n",
    "\n",
    "def sweep():\n",
    "    # Define the hyperparameters to tune using the wandb config object\n",
    "    config_defaults = {\n",
    "        'learning_rate': 5e-5,\n",
    "        'num_epochs': 5,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "\n",
    "    # Retrieve the hyperparameter values from wandb\n",
    "    config = wandb.config\n",
    "\n",
    "    # Train the model with the given hyperparameters\n",
    "    train(config)\n",
    "    \n",
    "wandb.agent(sweep_id, function=sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Best Model\n",
    "\n",
    "# Bert Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# #GPT2 Tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load('models/bert_model_5epochs.pth'))\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    encoded = tokenizer.encode_plus(text, add_special_tokens=True, padding='max_length',\n",
    "                                     truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device))\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        ai_probability = probabilities.detach().cpu().numpy()[0][0]\n",
    "        human_probability = probabilities.detach().cpu().numpy()[0][1]\n",
    "        total_probability = ai_probability + human_probability\n",
    "        ai_percentage = ai_probability / total_probability\n",
    "        human_percentage = human_probability / total_probability\n",
    "        print(\"\\n\")\n",
    "        print(f'Percentage of AI content: {ai_percentage*100:.2f}%')\n",
    "        print(f'Percentage of Human content: {human_percentage*100:.2f}%')\n",
    "        print(\"\\n\")\n",
    "        sentiment = 'AI-generated' if predictions.item() == 0 else 'Human-generated'\n",
    "    return sentiment\n",
    "\n",
    "# Get user input\n",
    "while True:\n",
    "    print(\"\\n\")\n",
    "    text = input(\"Enter a text to classify its sentiment (type 'quit' to exit): \\n \\n\")\n",
    "    if text.lower() == 'quit':\n",
    "        break\n",
    "    sentiment = get_sentiment(text)\n",
    "    print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
